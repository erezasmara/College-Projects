# Toxic Comments Classification CNN SGD
Project Structure:
```
├─ CNN_Classification
│  ├─ CNN_Model_SGD.h5
│  ├─ Final_Code_Toxic.py
│  ├─ Load_Cnn_Model.py
│  ├─ Loss&Acc.png
│  ├─ TrainData_Word_length.png
│  └─ TrainData_toxic_type.png
```
Description:
model that’s capable of detecting different types of of toxicity comments like threats, obscenity,  hate . base on CNN model

Requirement:
- Python 3.8 or above
- pip module
- data set train.csv & test.csv
- keras module
- tensorflow module


Installation with pip
```
## sudo python3 -m pip install -U keras
## sudo python3 -m pip install -U tensorflow
```
Start:

python3 Final_Code_Toxic.py

### Pictures:
![TrainData_Word_length](https://user-images.githubusercontent.com/33747218/137753240-f9cecaef-e8b9-4bd1-923d-912e344e290f.png)
![Loss Acc](https://user-images.githubusercontent.com/33747218/137753228-106a34fb-f981-43b1-a0a1-8b1b6076b1ed.png)
![TrainData_toxic_type](https://user-images.githubusercontent.com/33747218/137753235-911b5d86-b84c-41cc-bdc3-db3fa2bd0b74.png)


![](https://user-images.githubusercontent.com/33747218/137751284-2267f869-cb1f-44af-8ac9-a6b812bf94d7.png)

![](https://user-images.githubusercontent.com/33747218/137751283-a5222ff4-aa16-4646-9a38-9271d9cb8e65.png)

![](https://user-images.githubusercontent.com/33747218/137751274-f4f1f8c5-5cf0-4ab5-8a52-d1c9a80c4bcf.png)
